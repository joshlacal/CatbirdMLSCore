//
//  MLSGRDBManager.swift
//  Catbird
//
//  GRDB DatabasePool manager with SQLCipher encryption for MLS storage.
//  Uses DatabasePool for concurrent read access and serialized writes.
//  Actor isolation ensures thread-safe access with Swift 6 concurrency.
//

import Foundation
import GRDB
import OSLog

/// Manages encrypted GRDB DatabasePool instances with per-user isolation.
/// Actor provides thread-safe access and automatic isolation.
/// Uses DatabasePool for better read concurrency (multiple concurrent readers, single writer).
public actor MLSGRDBManager {

    // MARK: - Properties

    /// Shared singleton instance
    public static let shared = MLSGRDBManager()

    /// Logger for database operations
    private let logger = Logger(subsystem: "Catbird", category: "MLSGRDBManager")

    /// Active database pools per user DID (upgraded from DatabaseQueue for better concurrency)
    private var databases: [String: DatabasePool] = [:]

    /// Encryption manager
    private let encryption = MLSSQLCipherEncryption.shared

    /// Base directory for all user databases
    private let databaseDirectory: URL

    /// Database file extension
    private let fileExtension = "db"

    // MARK: - Initialization

    private init() {
        // Create base directory for MLS databases
        let appSupport: URL
        if let sharedContainer = FileManager.default.containerURL(
            forSecurityApplicationGroupIdentifier: "group.blue.catbird.shared")
        {
            appSupport = sharedContainer
        } else {
            appSupport =
                FileManager.default.urls(for: .applicationSupportDirectory, in: .userDomainMask)[0]
        }
        self.databaseDirectory = appSupport.appendingPathComponent("MLS", isDirectory: true)

        // Create directory if it doesn't exist
        do {
            try FileManager.default.createDirectory(
                at: databaseDirectory, withIntermediateDirectories: true)
        } catch {
            logger.error("Failed to create database directory: \(error.localizedDescription)")
        }
    }

    // MARK: - Public API

    /// Get or create encrypted DatabasePool for a user (actor isolation)
    /// - Parameter userDID: User's decentralized identifier
    /// - Returns: Encrypted GRDB DatabasePool
    /// - Throws: MLSSQLCipherError if creation fails
    public func getDatabasePool(for userDID: String) async throws -> DatabasePool {
        // Check cache first (actor isolation provides thread-safety)
        if let existingDatabase = databases[userDID] {
            return existingDatabase
        }

        // Check if database file already exists
        let dbPath = databasePath(for: userDID)
        let isNewDatabase = !FileManager.default.fileExists(atPath: dbPath.path)

        // Create/open database (runs off main thread via actor isolation)
        do {
            let database = try await createDatabase(for: userDID)

            // Cache the database (actor isolation provides thread-safety)
            databases[userDID] = database

            if isNewDatabase {
                logger.info("âœ¨ Created new database pool for user: \(userDID, privacy: .private)")
            } else {
                logger.info("ðŸ“‚ Opened existing database pool for user: \(userDID, privacy: .private)")
            }

            return database
        } catch {
            // If database creation fails with corruption error, attempt repair
            let errorDescription = error.localizedDescription
            if errorDescription.contains("out of memory") || errorDescription.contains("SQLITE") {
                logger.warning("âš ï¸ Database creation failed, attempting repair: \(errorDescription)")

                // Attempt repair
                try? repairDatabase(for: userDID)

                // Retry opening database after repair
                let database = try await createDatabase(for: userDID)

                databases[userDID] = database

                logger.info("âœ… Database recovered and reopened after repair for user: \(userDID, privacy: .private)")

                return database
            }

            // Re-throw if not a recoverable error
            throw error
        }
    }
    
    /// Legacy method for backwards compatibility - returns DatabaseQueue interface
    /// New code should use getDatabasePool(for:) instead
    @available(*, deprecated, renamed: "getDatabasePool(for:)", message: "Use getDatabasePool for better concurrency")
    public func getDatabaseQueue(for userDID: String) async throws -> DatabasePool {
        return try await getDatabasePool(for: userDID)
    }

    /// Close database for a user
    /// - Parameter userDID: User's decentralized identifier
    public func closeDatabase(for userDID: String) {
        if databases.removeValue(forKey: userDID) != nil {
            logger.info("Closed database for user: \(userDID, privacy: .private)")
        }
    }

    /// Close all databases
    func closeAllDatabases() {
        let userDIDs = Array(databases.keys)
        for userDID in userDIDs {
            databases.removeValue(forKey: userDID)
        }
        logger.info("Closed all databases")
    }

    /// Delete database file for a user (when removing account)
    /// - Parameter userDID: User's decentralized identifier
    /// - Throws: MLSSQLCipherError if deletion fails
    public func deleteDatabase(for userDID: String) async throws {
        // Close database first
        closeDatabase(for: userDID)

        // Delete database file
        let dbPath = databasePath(for: userDID)

        if FileManager.default.fileExists(atPath: dbPath.path) {
            do {
                try FileManager.default.removeItem(at: dbPath)
                logger.info("Deleted database for user: \(userDID, privacy: .private)")
            } catch {
                throw MLSSQLCipherError.databaseCreationFailed(underlying: error)
            }
        }

        // Delete encryption key
        try await encryption.deleteKey(for: userDID)
    }

    /// Repair corrupted database by removing WAL and SHM files
    /// Call this if you get SQLITE_NOMEM or other corruption errors
    /// - Parameter userDID: User's decentralized identifier
    public func repairDatabase(for userDID: String) throws {
        logger.warning("âš ï¸ Attempting to repair database for user: \(userDID, privacy: .private)")

        // Close database first
        closeDatabase(for: userDID)

        let dbPath = databasePath(for: userDID)
        let walPath = URL(fileURLWithPath: dbPath.path + "-wal")
        let shmPath = URL(fileURLWithPath: dbPath.path + "-shm")

        // Delete WAL and SHM files (will be recreated)
        for path in [walPath, shmPath] {
            if FileManager.default.fileExists(atPath: path.path) {
                do {
                    try FileManager.default.removeItem(at: path)
                    logger.info("Deleted corrupted file: \(path.lastPathComponent)")
                } catch {
                    logger.error("Failed to delete \(path.lastPathComponent): \(error.localizedDescription)")
                }
            }
        }

        logger.info("âœ… Database repair completed for user: \(userDID, privacy: .private)")
    }
    
    /// Force a WAL checkpoint to consolidate the WAL file into the main database
    /// This can help prevent memory exhaustion by reducing the size of auxiliary files
    /// - Parameter userDID: User's decentralized identifier
    public func checkpointDatabase(for userDID: String) async throws {
        guard let database = databases[userDID] else {
            logger.debug("No active database for checkpoint: \(userDID, privacy: .private)")
            return
        }
        
        try await database.write { db in
            // TRUNCATE mode checkpoints and then truncates the WAL file
            try db.execute(sql: "PRAGMA wal_checkpoint(TRUNCATE);")
        }
        
        logger.info("âœ… Database checkpoint completed for user: \(userDID, privacy: .private)")
    }

    /// Check if database exists for user
    /// - Parameter userDID: User's decentralized identifier
    /// - Returns: True if database file exists
    func databaseExists(for userDID: String) -> Bool {
        let dbPath = databasePath(for: userDID)
        return FileManager.default.fileExists(atPath: dbPath.path)
    }

    /// Get database file size
    /// - Parameter userDID: User's decentralized identifier
    /// - Returns: Size in bytes, or nil if database doesn't exist
    func databaseSize(for userDID: String) -> Int64? {
        let dbPath = databasePath(for: userDID)

        guard let attributes = try? FileManager.default.attributesOfItem(atPath: dbPath.path),
              let fileSize = attributes[.size] as? Int64
        else {
            return nil
        }

        return fileSize
    }

    // MARK: - Private Methods

    /// Create new encrypted database with GRDB DatabasePool (runs off main thread via actor isolation)
    private func createDatabase(for userDID: String) async throws -> DatabasePool {
        // Get or create encryption key
        let encryptionKey = try await encryption.getOrCreateKey(for: userDID)

        // Get database file path
        let dbPath = databasePath(for: userDID)

        // Configure GRDB with SQLCipher encryption
        var config = Configuration()
        
        // Enable busyMode for better concurrency handling
        config.busyMode = .timeout(5.0)  // Wait up to 5 seconds for locks
        
        // Note: defaultTransactionKind is automatically managed by GRDB
        
        // Enable readonly connections for readers (DatabasePool feature)
        config.readonly = false

        // Set QoS to userInitiated to match main thread priority
        // This prevents priority inversion when UI threads await database operations
        config.qos = .userInitiated
        
        // CRITICAL FIX: Limit maximum reader connections to prevent memory exhaustion
        // iOS has limited file descriptors and memory; too many concurrent readers
        // can exhaust SQLite's resources causing "out of memory" errors
        // Default is 5, but we limit to 4 to leave headroom for the writer
        config.maximumReaderCount = 4

        // Configure encryption using GRDB's prepareDatabase
        config.prepareDatabase { db in
            // Convert key to hex string for SQLCipher
            // SQLCipher expects: PRAGMA key = "x'hexstring'"
            let hexKey = encryptionKey.map { String(format: "%02x", $0) }.joined()

            // Set encryption key using quoted hex literal format
            try db.execute(sql: "PRAGMA key = \"x'\(hexKey)'\";")

            // Try SQLCipher 4 settings first
            try db.execute(sql: "PRAGMA cipher_page_size = 4096;")
            try db.execute(sql: "PRAGMA kdf_iter = 256000;")
            try db.execute(sql: "PRAGMA cipher_hmac_algorithm = HMAC_SHA512;")
            try db.execute(sql: "PRAGMA cipher_kdf_algorithm = PBKDF2_HMAC_SHA512;")

            // Test if database can be read with these settings
            do {
                _ = try Int.fetchOne(db, sql: "SELECT count(*) FROM sqlite_master;")
            } catch {
                // If SQLCipher 4 settings fail, try SQLCipher 3 compatibility mode
                // This handles existing databases created with older settings
                try db.execute(sql: "PRAGMA cipher_page_size = 1024;")
                try db.execute(sql: "PRAGMA kdf_iter = 64000;")
                try db.execute(sql: "PRAGMA cipher_hmac_algorithm = HMAC_SHA1;")
                try db.execute(sql: "PRAGMA cipher_kdf_algorithm = PBKDF2_HMAC_SHA1;")

                // Verify it works now
                _ = try Int.fetchOne(db, sql: "SELECT count(*) FROM sqlite_master;")
            }

            // Enable WAL mode for better concurrency (critical for DatabasePool)
            try db.execute(sql: "PRAGMA journal_mode = WAL;")
            // Reduce checkpoint threshold to prevent WAL file from growing too large
            // (1000 pages * 4KB = 4MB threshold before automatic checkpoint)
            try db.execute(sql: "PRAGMA wal_autocheckpoint = 1000;")
            try db.execute(sql: "PRAGMA synchronous = NORMAL;")  // NORMAL is sufficient with WAL
            
            // Limit SQLite's page cache to prevent memory bloat
            // Negative value = KB, so -2000 = 2MB cache per connection
            // With 4 reader connections + 1 writer = ~10MB total cache (reasonable for iOS)
            try db.execute(sql: "PRAGMA cache_size = -2000;")

            // Enable foreign keys
            try db.execute(sql: "PRAGMA foreign_keys = ON;")
            
            // Memory-mapped I/O disabled to prevent "out of memory" errors on iOS
            // iOS has strict memory limits; large mmap regions can exhaust available memory
            // especially when multiple database connections are open during polling/sync
            // The default mmap_size of 0 (disabled) is safe and performant enough
            // try db.execute(sql: "PRAGMA mmap_size = 268435456;")  // DISABLED - was 256MB
        }

        // Create DatabasePool for concurrent reads
        let database: DatabasePool
        do {
            database = try DatabasePool(path: dbPath.path, configuration: config)
        } catch {
            throw MLSSQLCipherError.databaseCreationFailed(underlying: error)
        }

        // Set file protection (iOS Data Protection)
        try setFileProtection(for: dbPath)

        // Exclude from backups
        try excludeFromBackup(dbPath)

        // Run migrations
        try runMigrations(database)

        return database
    }

    /// Run database migrations using DatabaseMigrator
    private func runMigrations(_ db: DatabasePool) throws {
        var migrator = DatabaseMigrator()

        // MARK: v1 - Initial schema
        migrator.registerMigration("v1_initial_schema") { db in
      // Create all MLS tables using GRDB's native table creation
      try db.create(table: "MLSConversationModel") { t in
        t.primaryKey("conversationID", .text).notNull()
        t.column("currentUserDID", .text).notNull()
        t.column("groupID", .blob).notNull()
        t.column("epoch", .integer).notNull().defaults(to: 0)
        t.column("title", .text)
        t.column("avatarURL", .text)
        t.column("createdAt", .datetime).notNull()
        t.column("updatedAt", .datetime).notNull()
        t.column("lastMessageAt", .datetime)
        t.column("isActive", .boolean).notNull().defaults(to: true)
        t.column("needsRejoin", .boolean).notNull().defaults(to: false)
        t.column("rejoinRequestedAt", .datetime)
      }

      try db.create(table: "MLSMessageModel") { t in
        t.primaryKey("messageID", .text).notNull()
        t.column("currentUserDID", .text).notNull()
        t.column("conversationID", .text).notNull().references(
          "MLSConversationModel", onDelete: .cascade)
        t.column("senderID", .text).notNull()
        t.column("plaintext", .text)
        t.column("embedData", .blob)
        t.column("wireFormat", .blob)
        t.column("contentType", .text).notNull()
        t.column("timestamp", .datetime).notNull()
        t.column("epoch", .integer).notNull()
        t.column("sequenceNumber", .integer).notNull()
        t.column("authenticatedData", .blob)
        t.column("signature", .blob)
        t.column("isDelivered", .boolean).notNull().defaults(to: false)
        t.column("isRead", .boolean).notNull().defaults(to: false)
        t.column("isSent", .boolean).notNull().defaults(to: false)
        t.column("sendAttempts", .integer).notNull().defaults(to: 0)
        t.column("error", .text)
        t.column("processingState", .text).notNull()
        t.column("gapBefore", .boolean).notNull().defaults(to: false)
        t.column("plaintextExpired", .boolean).notNull().defaults(to: false)
      }

      try db.create(table: "MLSMemberModel") { t in
        t.primaryKey("memberID", .text).notNull()
        t.column("conversationID", .text).notNull().references(
          "MLSConversationModel", onDelete: .cascade)
        t.column("currentUserDID", .text).notNull()
        t.column("did", .text).notNull()
        t.column("handle", .text)
        t.column("displayName", .text)
        t.column("leafIndex", .integer).notNull()
        t.column("credentialData", .blob)
        t.column("signaturePublicKey", .blob)
        t.column("addedAt", .datetime).notNull()
        t.column("updatedAt", .datetime).notNull()
        t.column("removedAt", .datetime)
        t.column("isActive", .boolean).notNull().defaults(to: true)
        t.column("role", .text).notNull()
        t.column("capabilities", .blob)
      }

      try db.create(table: "MLSKeyPackageModel") { t in
        t.primaryKey("keyPackageID", .text).notNull()
        t.column("currentUserDID", .text).notNull()
        t.column("keyPackageData", .blob).notNull()
        t.column("credentialData", .blob).notNull()
        t.column("createdAt", .datetime).notNull()
        t.column("expiresAt", .datetime)
        t.column("isPublished", .boolean).notNull().defaults(to: false)
        t.column("isUsed", .boolean).notNull().defaults(to: false)
      }

      try db.create(table: "MLSEpochKeyModel") { t in
        t.primaryKey("epochKeyID", .text).notNull()
        t.column("conversationID", .text).notNull().references(
          "MLSConversationModel", onDelete: .cascade)
        t.column("currentUserDID", .text).notNull()
        t.column("epoch", .integer).notNull()
        t.column("keyMaterial", .blob).notNull()
        t.column("createdAt", .datetime).notNull()
        t.column("expiresAt", .datetime)
        t.column("isActive", .boolean).notNull().defaults(to: true)
      }

      try db.create(table: "MLSMessageReactionModel") { t in
        t.primaryKey("reactionID", .text).notNull()
        t.column("messageID", .text).notNull().references("MLSMessageModel", onDelete: .cascade)
        t.column("conversationID", .text).notNull()
        t.column("currentUserDID", .text).notNull()
        t.column("actorDID", .text).notNull()
        t.column("emoji", .text).notNull()
        t.column("action", .text).notNull()
        t.column("timestamp", .datetime).notNull()
      }

      try db.create(table: "MLSStorageBlobModel") { t in
        t.primaryKey("blobID", .text).notNull()
        t.column("conversationID", .text).references("MLSConversationModel", onDelete: .cascade)
        t.column("currentUserDID", .text).notNull()
        t.column("blobType", .text).notNull()
        t.column("blobData", .blob).notNull()
        t.column("mimeType", .text).notNull()
        t.column("size", .integer).notNull()
        t.column("createdAt", .datetime).notNull()
        t.column("updatedAt", .datetime).notNull()
      }

      try db.create(table: "MLSReportModel") { t in
        t.primaryKey("id", .text).notNull()
        t.column("convo_id", .text).notNull().references("MLSConversationModel", onDelete: .cascade)
        t.column("reporter_did", .text).notNull()
        t.column("reported_did", .text).notNull()
        t.column("reason", .text).notNull()
        t.column("details", .text)
        t.column("status", .text).notNull().defaults(to: "pending")
        t.column("action", .text)
        t.column("resolution_notes", .text)
        t.column("created_at", .datetime).notNull()
        t.column("resolved_at", .datetime)
      }

      try db.create(table: "MLSAdminRosterModel") { t in
        t.primaryKey("convo_id", .text).notNull().references(
          "MLSConversationModel", onDelete: .cascade)
        t.column("version", .integer).notNull()
        t.column("roster_hash", .text).notNull()
        t.column("encrypted_roster", .blob).notNull()
        t.column("updated_at", .datetime).notNull()
      }
    }

    // MARK: v2 - Performance indexes
    migrator.registerMigration("v2_performance_indexes") { db in
      // MLSMessageModel indexes - frequently queried by conversation and timestamp
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_conversation_timestamp
            ON MLSMessageModel(conversationID, timestamp DESC);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_user
            ON MLSMessageModel(currentUserDID);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_sender
            ON MLSMessageModel(senderID);
          """)

      // MLSConversationModel indexes - frequently queried by user and activity
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_conversation_user_active
            ON MLSConversationModel(currentUserDID, isActive, lastMessageAt DESC);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_conversation_updated
            ON MLSConversationModel(updatedAt DESC);
          """)

      // MLSMemberModel indexes - frequently queried by conversation
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_member_conversation
            ON MLSMemberModel(conversationID);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_member_did
            ON MLSMemberModel(did);
          """)

      // MLSEpochKeyModel indexes - queried by conversation and epoch
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_epoch_conversation_epoch
            ON MLSEpochKeyModel(conversationID, epoch DESC);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_epoch_created
            ON MLSEpochKeyModel(createdAt DESC);
          """)

      // MLSMessageReactionModel indexes - queried by message
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_reaction_message
            ON MLSMessageReactionModel(messageID);
          """)

      // MLSStorageBlobModel indexes - queried by user and type
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_blob_user_type
            ON MLSStorageBlobModel(currentUserDID, blobType);
          """)

      // MLSReportModel indexes - frequently queried by conversation and status
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_reports_convo
            ON MLSReportModel(convo_id);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_reports_status
            ON MLSReportModel(status);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_reports_convo_status
            ON MLSReportModel(convo_id, status, created_at DESC);
          """)

      // MLSAdminRosterModel - primary key index already covers convo_id lookups
    }

    // MARK: v3 - Consumption tracking
    migrator.registerMigration("v3_consumption_tracking") { db in
      // Create consumption record table
      try db.create(table: "MLSConsumptionRecordModel") { t in
        t.primaryKey("recordID", .text).notNull()
        t.column("currentUserDID", .text).notNull()
        t.column("timestamp", .datetime).notNull()
        t.column("packagesConsumed", .integer).notNull()
        t.column("operation", .text).notNull()
        t.column("context", .text)
      }

      // Indexes for consumption queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_consumption_user_timestamp
            ON MLSConsumptionRecordModel(currentUserDID, timestamp DESC);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_consumption_operation
            ON MLSConsumptionRecordModel(currentUserDID, operation);
          """)
    }

    // MARK: v4 - Fix message ordering index
    migrator.registerMigration("v4_fix_message_ordering_index") { db in
      // Drop old timestamp-based index that doesn't match query ordering
      try db.execute(
        sql: """
            DROP INDEX IF EXISTS idx_message_conversation_timestamp;
          """)

      // Create new index matching actual query ordering (epoch, sequenceNumber)
      // This prevents messages from appearing out of order during epoch transitions
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_conversation_epoch_seq
            ON MLSMessageModel(conversationID, epoch ASC, sequenceNumber ASC);
          """)

      // Keep separate timestamp index for catchup and time-based queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_timestamp
            ON MLSMessageModel(timestamp DESC);
          """)
    }

    // MARK: v5 - Error tracking and recovery fields
    migrator.registerMigration("v5_error_tracking_recovery") { db in
      // Add error tracking fields to MLSMessageModel
      try db.execute(
        sql: """
            ALTER TABLE MLSMessageModel
            ADD COLUMN processingError TEXT;
          """)

      try db.execute(
        sql: """
            ALTER TABLE MLSMessageModel
            ADD COLUMN processingAttempts INTEGER NOT NULL DEFAULT 0;
          """)

      try db.execute(
        sql: """
            ALTER TABLE MLSMessageModel
            ADD COLUMN validationFailureReason TEXT;
          """)

      // Add recovery tracking fields to MLSConversationModel
      try db.execute(
        sql: """
            ALTER TABLE MLSConversationModel
            ADD COLUMN lastRecoveryAttempt DATETIME;
          """)

      try db.execute(
        sql: """
            ALTER TABLE MLSConversationModel
            ADD COLUMN consecutiveFailures INTEGER NOT NULL DEFAULT 0;
          """)

      // Create index for finding conversations needing recovery
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_conversation_recovery
            ON MLSConversationModel(consecutiveFailures DESC, lastRecoveryAttempt);
          """)

      // Create index for finding messages with processing errors
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_message_processing_error
            ON MLSMessageModel(conversationID, processingError);
          """)
    }

    // MARK: v6 - Membership history and visibility tracking
    migrator.registerMigration("v6_membership_history") { db in
      // Add membership change tracking to MLSConversationModel
      try db.execute(
        sql: """
            ALTER TABLE MLSConversationModel
            ADD COLUMN lastMembershipChangeAt DATETIME;
          """)

      try db.execute(
        sql: """
            ALTER TABLE MLSConversationModel
            ADD COLUMN unacknowledgedMemberChanges INTEGER NOT NULL DEFAULT 0;
          """)

      // Check if MLSMemberModel needs removedBy and removalReason columns
      // (these were added in Phase 1.2, but migration handles databases without them)
      let hasRemovedBy = try db.columns(in: "MLSMemberModel")
        .contains { $0.name == "removedBy" }
      let hasRemovalReason = try db.columns(in: "MLSMemberModel")
        .contains { $0.name == "removalReason" }

      if !hasRemovedBy {
        try db.execute(
          sql: """
              ALTER TABLE MLSMemberModel
              ADD COLUMN removedBy TEXT;
            """)
      }

      if !hasRemovalReason {
        try db.execute(
          sql: """
              ALTER TABLE MLSMemberModel
              ADD COLUMN removalReason TEXT;
            """)
      }

      // Create membership event audit log table
      try db.create(table: "MLSMembershipEventModel") { t in
        t.primaryKey("id", .text).notNull()
        t.column("conversationID", .text).notNull()
          .references("MLSConversationModel", onDelete: .cascade)
        t.column("currentUserDID", .text).notNull()
        t.column("memberDID", .text).notNull()
        t.column("eventType", .text).notNull()
        t.column("timestamp", .datetime).notNull()
        t.column("actorDID", .text)
        t.column("epoch", .integer).notNull()
        t.column("metadata", .blob)
      }

      // Create indexes for membership event queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_membership_events_conversation_timestamp
            ON MLSMembershipEventModel(conversationID, timestamp DESC);
          """)

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_membership_events_member
            ON MLSMembershipEventModel(memberDID, timestamp DESC);
          """)
    }

    // MARK: v7 - Roster snapshots and tree hash pinning for E2EE validation
    migrator.registerMigration("v7_validation_hardening") { db in
      // Create roster snapshot table for membership change detection
      try db.create(table: "MLSRosterSnapshotModel") { t in
        t.primaryKey("snapshotID", .text).notNull()
        t.column("conversationID", .text).notNull()
          .references("MLSConversationModel", onDelete: .cascade)
        t.column("epoch", .integer).notNull()
        t.column("memberDIDs", .blob).notNull()  // JSON-encoded [String]
        t.column("treeHash", .blob)
        t.column("timestamp", .datetime).notNull()
        t.column("previousSnapshotID", .text)
      }

      // Create indexes for roster snapshot queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_roster_snapshots_conversation_epoch
            ON MLSRosterSnapshotModel(conversationID, epoch DESC);
          """)

      try db.execute(
        sql: """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_roster_snapshots_conversation_epoch_unique
            ON MLSRosterSnapshotModel(conversationID, epoch);
          """)

      // Create tree hash pin table for state divergence detection
      try db.create(table: "MLSTreeHashPinModel") { t in
        t.primaryKey("pinID", .text).notNull()
        t.column("conversationID", .text).notNull()
          .references("MLSConversationModel", onDelete: .cascade)
        t.column("epoch", .integer).notNull()
        t.column("treeHash", .blob).notNull()
        t.column("pinnedAt", .datetime).notNull()
        t.column("source", .text).notNull()
        t.column("verified", .boolean).notNull().defaults(to: false)
      }

      // Create indexes for tree hash pin queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_tree_hash_pins_conversation_epoch
            ON MLSTreeHashPinModel(conversationID, epoch DESC);
          """)

      try db.execute(
        sql: """
            CREATE UNIQUE INDEX IF NOT EXISTS idx_tree_hash_pins_conversation_epoch_unique
            ON MLSTreeHashPinModel(conversationID, epoch);
          """)

      // Create validation audit log table for security decisions
      try db.create(table: "MLSValidationAuditLog") { t in
        t.primaryKey("id", .text).notNull()
        t.column("conversationID", .text).notNull()
        t.column("timestamp", .datetime).notNull()
        t.column("operationType", .text).notNull()
        t.column("credentialDID", .text)
        t.column("epoch", .integer).notNull()
        t.column("decision", .text).notNull()  // "allowed", "denied", "requires_approval"
        t.column("reason", .text)
        t.column("metadata", .blob)
      }

      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_validation_audit_conversation_timestamp
            ON MLSValidationAuditLog(conversationID, timestamp DESC);
          """)
    }

    // MARK: v8 - Add deletedAt to MLSEpochKeyModel
    migrator.registerMigration("v8_add_deleted_at_to_epoch_keys") { db in
      // Add deletedAt column to MLSEpochKeyModel if it doesn't exist
      let hasDeletedAt = try db.columns(in: "MLSEpochKeyModel")
        .contains { $0.name == "deletedAt" }

      if !hasDeletedAt {
        try db.execute(
          sql: """
              ALTER TABLE MLSEpochKeyModel
              ADD COLUMN deletedAt DATETIME;
            """)
      }

      // Add index for cleanup queries
      try db.execute(
        sql: """
            CREATE INDEX IF NOT EXISTS idx_epoch_deleted
            ON MLSEpochKeyModel(currentUserDID, deletedAt);
          """)
    }

    // Execute all migrations
    try migrator.migrate(db)
  }

  /// Set iOS Data Protection on database file
  private func setFileProtection(for fileURL: URL) throws {
    #if targetEnvironment(macCatalyst)
    // Skip file protection on Mac Catalyst - macOS has its own security model (FileVault, etc.)
    logger.debug("Skipping file protection on Mac Catalyst")
    #else
    do {
      try FileManager.default.setAttributes(
        [.protectionKey: FileProtectionType.complete],
        ofItemAtPath: fileURL.path
      )
    } catch {
      throw MLSSQLCipherError.fileProtectionFailed(underlying: error)
    }
    #endif
  }

  /// Exclude database from iCloud/iTunes backup
  private func excludeFromBackup(_ fileURL: URL) throws {
    var url = fileURL
    do {
      var resourceValues = URLResourceValues()
      resourceValues.isExcludedFromBackup = true
      try url.setResourceValues(resourceValues)
    } catch {
      throw MLSSQLCipherError.backupExclusionFailed(underlying: error)
    }
  }

  /// Sanitize DID for filesystem compatibility
  /// Removes characters that are problematic in file paths
  private func sanitizeDID(_ userDID: String) -> String {
    userDID
      .replacingOccurrences(of: ":", with: "-")
      .replacingOccurrences(of: "/", with: "-")
      .replacingOccurrences(of: "#", with: "-")
      .replacingOccurrences(of: "?", with: "-")
  }

  /// Get database file path for user
  private func databasePath(for userDID: String) -> URL {
    // Sanitize DID string for use in filesystem path (doesn't modify the actual DID)
    let sanitizedDID = sanitizeDID(userDID)

    // Create filename: mls_messages_{DID}.db
    let filename = "mls_messages_\(sanitizedDID).\(fileExtension)"

    return databaseDirectory.appendingPathComponent(filename)
  }
}

// MARK: - Debug Helpers

#if DEBUG
    extension MLSGRDBManager {
        /// List all database files
        func listDatabases() -> [String] {
            guard let files = try? FileManager.default.contentsOfDirectory(atPath: databaseDirectory.path)
            else {
                return []
            }

            return files.filter { $0.hasSuffix(".\(fileExtension)") }
        }

        /// Get table statistics for a user's database
        func getTableStats(for userDID: String) async throws -> [String: Int] {
            let db = try await getDatabasePool(for: userDID)

            return try await db.read { database in
                var stats: [String: Int] = [:]

                // Get all table names
                let tables = try String.fetchAll(
                    database,
                    sql: "SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';"
                )

                // Count rows in each table
                for table in tables {
                    if let count = try Int.fetchOne(database, sql: "SELECT COUNT(*) FROM \(table);") {
                        stats[table] = count
                    }
                }

                return stats
            }
        }

        /// Export database to unencrypted file (for debugging only)
        func exportUnencrypted(for userDID: String, to destinationPath: String) async throws {
            let db = try await getDatabasePool(for: userDID)

            try await db.write { database in
                // Use SQLCipher's ATTACH and export
                try database.execute(
                    sql: """
                        ATTACH DATABASE '\(destinationPath)' AS plaintext KEY '';
                        SELECT sqlcipher_export('plaintext');
                        DETACH DATABASE plaintext;
                      """)
            }

            logger.warning(
                "âš ï¸ Exported UNENCRYPTED database to \(destinationPath) - DELETE after debugging!")
        }

        /// Verify database encryption
        func verifyEncryption(for userDID: String) async throws -> Bool {
            let db = try await getDatabasePool(for: userDID)

            // Try to query sqlite_master (should succeed with correct key)
            return try await db.read { database in
                _ = try Int.fetchOne(database, sql: "SELECT COUNT(*) FROM sqlite_master;")
                return true
            }
        }

        /// Run integrity check on database
        func checkIntegrity(for userDID: String) async throws -> Bool {
            let db = try await getDatabasePool(for: userDID)

            return try await db.read { database in
                if let result = try String.fetchOne(database, sql: "PRAGMA integrity_check;") {
                    return result == "ok"
                }
                return false
            }
        }

        /// Optimize database (vacuum and analyze)
        func optimize(for userDID: String) async throws {
            let db = try await getDatabasePool(for: userDID)

            try await db.write { database in
                try database.execute(sql: "VACUUM;")
                try database.execute(sql: "ANALYZE;")
            }

            logger.info("Optimized database for user: \(userDID, privacy: .private)")
        }
    }
#endif
